{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e0c903-a74f-432a-821c-297282e9d161",
   "metadata": {},
   "source": [
    "## Laden der API Keys über eine .env-Datei\n",
    "\n",
    "In diesem Abschnitt laden wir die benötigten API-Schlüssel für OpenAI und OpenRouter aus einer `.env`-Datei. Dies ermöglicht es uns, sensible Zugangsdaten sicher zu verwalten, ohne sie direkt im Code zu hinterlegen.\n",
    "\n",
    "Die Bibliothek `python-dotenv` liest die `.env`-Datei aus dem Projektverzeichnis und stellt die darin definierten Umgebungsvariablen zur Verfügung. Die `.env`-Datei sollte folgendes Format haben:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "OPEN_ROUTER_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "**Wichtig:** Die `.env`-Datei sollte in der `.gitignore` aufgeführt werden, um ein versehentliches Commit von API-Schlüsseln zu verhindern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601ed5b-b145-4eea-acde-bb6023f6af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82dac41-09ac-45ae-9802-1c386f181028",
   "metadata": {},
   "source": [
    "## Laden der erforderlichen Abhängigkeiten\n",
    "\n",
    "Hier importieren wir alle benötigten Python-Bibliotheken für die Arbeit mit verschiedenen LLM-APIs:\n",
    "\n",
    "- **`openai`**: Die offizielle OpenAI-Client-Bibliothek, die wir auch für kompatible APIs (LM-Studio, OpenRouter) verwenden\n",
    "- **`IPython.display`**: Für die formatierte Ausgabe von Markdown-Inhalten im Notebook\n",
    "- **`json`**: Zum Parsen von JSON-Antworten des Judge-Modells\n",
    "- **`os`**: Für den Zugriff auf Umgebungsvariablen (API-Schlüssel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8975c-b040-48e1-b9f9-4f9f9b049671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e5cf2-ec29-4faf-9b73-f5311535801e",
   "metadata": {},
   "source": [
    "## Initialisierung der LLM-Clients\n",
    "\n",
    "In diesem Abschnitt erstellen wir drei verschiedene API-Clients, die alle die OpenAI-kompatible Schnittstelle nutzen:\n",
    "\n",
    "1. **`openai_client`**: Direkter Zugang zur OpenAI-API (GPT-4, GPT-4o, etc.)\n",
    "   - Verwendet den API-Key aus der Umgebungsvariable `OPENAI_API_KEY`\n",
    "\n",
    "2. **`lmstudio_client`**: Verbindung zu einem lokalen LM-Studio-Server\n",
    "   - Nutzt eine custom `base_url` zum lokalen Server (hier: `http://100.115.26.126:1234/v1`)\n",
    "   - Ermöglicht die Nutzung von lokalen Open-Source-Modellen (DeepSeek, LLaMA, etc.)\n",
    "\n",
    "3. **`or_client`**: Zugang zu OpenRouter für alternative Modelle\n",
    "   - OpenRouter bietet Zugriff auf verschiedene Modelle (Claude, Gemini, etc.) über eine einheitliche API\n",
    "   - Verwendet den API-Key aus der Umgebungsvariable `OPEN_ROUTER_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68039bab-7bf0-4362-84ab-4aef678b0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "lmstudio_client = OpenAI(\n",
    "    base_url=\"http://100.115.26.126:1234/v1\",  # LLM Studio Server\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "or_client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.getenv(\"OPEN_ROUTER_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556426d6-8911-4ef7-8069-f3ea2282e9da",
   "metadata": {},
   "source": [
    "## Definition der Prompts für die Dockerfile-Analyse\n",
    "\n",
    "Hier definieren wir die verschiedenen Prompts, die wir für die LLM-Interaktion verwenden:\n",
    "\n",
    "- **`SYSTEM_PROMPT`**: Legt die Rolle des LLM als DevSecOps-Experten fest und gibt den Kontext für alle nachfolgenden Anfragen vor\n",
    "\n",
    "- **`CREATE_PROMPT`**: Fordert das LLM auf, ein produktionsreifes Dockerfile für eine Flask-Anwendung zu erstellen\n",
    "  - Fokus auf Best Practices wie Multi-Stage-Builds und minimale Base-Images\n",
    "  - Soll kommentiert und nachvollziehbar sein\n",
    "\n",
    "- **`REVIEW_PROMPT`**: Template für die Bewertung bestehender Dockerfiles\n",
    "  - Wird mit einem konkreten Dockerfile gefüllt (via `.format()`)\n",
    "  - Fordert konkrete Verbesserungsvorschläge mit Begründung an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c9b39-c2f2-49df-96de-f5129e4ae357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Du bist ein erfahrener DevSecOps-Experte mit Fokus auf sichere Containerisierung.\n",
    "\"\"\"\n",
    "\n",
    "### Bewerte und optimiere Dockerfiles nach folgenden Kriterien:\n",
    "### 1) Sicherheit (least privilege, non-root, Patching, Supply Chain, CVE-Risiko)\n",
    "### 2) Performance / Image-Größe (Layer, Cache, Multi-Stage, Pinning)\n",
    "### 3) Best Practices für Production (Signals, Healthcheck, Entrypoint/CMD, WSGI/ASGI)\n",
    "### 4) Nachvollziehbarkeit (klare Schritte, Kommentare)\n",
    "\n",
    "\n",
    "CREATE_PROMPT = \"\"\"\n",
    "Erstelle ein sauberes, effizientes Dockerfile für eine einfache Flask‑Webanwendung mit Python 3.11.\n",
    "Nutze Best Practices (mehrstufiger Build, minimale Base‑Images) und erkläre im Kommentar kurz die Schritte.\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_PROMPT = (\n",
    "    \"Überprüfe das folgende Dockerfile auf Verbesserungsmöglichkeiten.\"\n",
    "    \"Schlage Optimierungen (Sicherheit, Größe, Performance) vor und begründe sie.\\n\\n{}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d103f0-2454-42f6-9671-d6e3af0eb658",
   "metadata": {},
   "source": [
    "## Helper-Funktion für LLM-Anfragen\n",
    "\n",
    "Die Funktion `ask_openai()` ist unsere zentrale Schnittstelle zu den verschiedenen LLM-APIs:\n",
    "\n",
    "**Parameter:**\n",
    "- `client`: Der zu verwendende API-Client (openai_client, lmstudio_client oder or_client)\n",
    "- `model_name`: Name des Modells (z.B. \"gpt-4o\", \"deepseek-r1-distill-qwen-7b\", \"anthropic/claude-opus-4.1\")\n",
    "- `prompt`: Die konkrete Aufgabenstellung für das Modell\n",
    "\n",
    "**Funktionsweise:**\n",
    "- Sendet eine Chat-Completion-Anfrage mit dem System-Prompt und User-Prompt\n",
    "- Verwendet `temperature=0.2` für konsistente, deterministische Antworten\n",
    "- Gibt die Textantwort des Modells zurück\n",
    "\n",
    "Diese Funktion abstrahiert die API-Kommunikation und macht es einfach, zwischen verschiedenen Modellen zu wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8e321-d422-4a21-8391-565ca75f97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(client, model_name, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd617e",
   "metadata": {},
   "source": [
    "## Dockerfile-Generierung mit verschiedenen LLM-Modellen\n",
    "\n",
    "In den folgenden Zellen lassen wir vier verschiedene LLM-Modelle jeweils ein Dockerfile für eine Flask-Webanwendung erstellen. Dies ermöglicht uns einen direkten Vergleich der verschiedenen Ansätze und Qualitäten:\n",
    "\n",
    "1. **GPT-4o (OpenAI)**: Das neueste und leistungsfähigste Modell von OpenAI mit exzellentem Verständnis für DevSecOps-Praktiken\n",
    "\n",
    "2. **LLaMA 3.2 3B (lokal via LM-Studio)**: Ein kleineres Open-Source-Modell, das lokal ausgeführt wird - interessant für Datenschutz und Kosteneffizienz\n",
    "\n",
    "3. **Claude Opus 4.1 (via OpenRouter)**: Anthropics Top-Modell, bekannt für detaillierte und durchdachte Antworten\n",
    "\n",
    "4. **DeepSeek R1 Distill Qwen 7B (lokal via LM-Studio)**: Ein spezialisiertes Reasoning-Modell, das schrittweise denkt\n",
    "\n",
    "Jedes Modell erhält denselben `CREATE_PROMPT` und soll ein produktionsreifes, sicheres Dockerfile mit Best Practices erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2c3b8-bc2c-4fa2-95a0-e55f2ecd7522",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dockerfile = ask_openai(openai_client, \"gpt-4o\", CREATE_PROMPT)\n",
    "print(\"\\n==== GPT‑4 – generiertes Dockerfile ===\\n\")\n",
    "display(Markdown(gpt_dockerfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340530f-43c2-44f2-a4dc-0a8bf47ae5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llama_dockerfile = ask_openai(lmstudio_client, \"llama-3.2-3b-instruct\", CREATE_PROMPT)\n",
    "print(\"\\n==== LLaMA – generiertes Dockerfile ===\\n\")\n",
    "display(Markdown(local_llama_dockerfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a0316-eb18-42f6-a243-ca17e35a12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opus = ask_openai(or_client, \"anthropic/claude-opus-4.1\", CREATE_PROMPT)\n",
    "print(\"\\n==== Opus – generiertes Dockerfile ===\\n\")\n",
    "display(Markdown(opus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb82f4d-cc80-4c55-9ced-198d3900418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ds_dockerfile = ask_openai(lmstudio_client, \"deepseek-r1-distill-qwen-7b\", CREATE_PROMPT)\n",
    "print(\"\\n==== Deepseek – generiertes Dockerfile ===\\n\")\n",
    "display(Markdown(local_ds_dockerfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d3ff8",
   "metadata": {},
   "source": [
    "## Review eines generierten Dockerfiles\n",
    "\n",
    "Nachdem wir Dockerfiles erstellen lassen haben, nutzen wir nun den `REVIEW_PROMPT`, um ein bestehendes Dockerfile analysieren und verbessern zu lassen. \n",
    "\n",
    "In diesem Beispiel:\n",
    "- Nehmen wir das von GPT-4o generierte Dockerfile (`gpt_dockerfile`)\n",
    "- Übergeben es an GPT-4o selbst zur kritischen Überprüfung\n",
    "- Fordern konkrete Verbesserungsvorschläge mit Begründung an\n",
    "\n",
    "Dies demonstriert einen wichtigen DevSecOps-Workflow: Selbst automatisch generierte Dockerfiles sollten einem Review unterzogen werden, um:\n",
    "- Sicherheitslücken zu identifizieren\n",
    "- Performance-Optimierungen zu finden\n",
    "- Best Practices durchzusetzen\n",
    "- Die Image-Größe zu reduzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f1a1d5-2cdf-4871-aa82-8f96508be638",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_gpt = ask_openai(openai_client, \"gpt-4o\", REVIEW_PROMPT.format(gpt_dockerfile))\n",
    "display(Markdown(review_gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_opus_ds = ask_openai(or_client, \"anthropic/claude-opus-4.1\", REVIEW_PROMPT.format(local_ds_dockerfile))\n",
    "display(Markdown(review_opus_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad733cb",
   "metadata": {},
   "source": [
    "## LLM as Judge: Automatisierte Bewertung der Dockerfile-Qualität\n",
    "\n",
    "Um die verschiedenen von den LLMs generierten Dockerfiles objektiv zu vergleichen, implementieren wir das **\"LLM as Judge\"**-Prinzip. Dabei nutzen wir ein leistungsstarkes LLM (hier: Claude Opus 4.1), um die Ausgaben der anderen Modelle zu bewerten.\n",
    "\n",
    "### Funktionsweise:\n",
    "\n",
    "1. **Judge-System-Prompt (`JUDGE_SYSTEM_PROMPT`)**: \n",
    "   - Definiert das LLM als unparteiischen technischen Gutachter\n",
    "   - Legt 5 Bewertungskriterien fest (correctness, security, best_practices, clarity, actionability)\n",
    "   - Fordert strukturierte JSON-Ausgabe mit Scores von 0-5\n",
    "\n",
    "2. **Workflow der Bewertung**:\n",
    "   - `build_judge_prompt()`: Kombiniert die ursprüngliche Aufgabe mit allen Modellantworten\n",
    "   - `evaluate_models_with_openai_judge()`: Sendet die Anfrage an das Judge-Modell und parst das JSON\n",
    "   - `markdown_score_table()`: Erstellt eine übersichtliche Tabelle mit Scores und Ranking\n",
    "   - `compare_answers_markdown()`: High-Level-Funktion, die alles zusammenführt\n",
    "\n",
    "3. **Bewertungskriterien**:\n",
    "   - **Correctness**: Technische Korrektheit der Dockerfile-Syntax und Befehle\n",
    "   - **Security**: Sicherheitsaspekte (Non-Root, Updates, Supply Chain)\n",
    "   - **Best Practices**: Production-Ready-Features (Multi-Stage, Layering, Healthchecks)\n",
    "   - **Clarity**: Verständlichkeit und Nachvollziehbarkeit\n",
    "   - **Actionability**: Konkrete, direkt umsetzbare Vorschläge\n",
    "\n",
    "### Vorteil:\n",
    "Statt subjektiv zu entscheiden, welches Dockerfile besser ist, erhalten wir eine datenbasierte, reproduzierbare Bewertung mit detailliertem Feedback zu jedem Kriterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7bbf18-0f42-456d-8e31-8987b1aa81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM as Judge\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Neutraler System-Prompt für den Judge\n",
    "# ------------------------------------------------------------\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "Du bist ein unparteiischer, sehr strenger technischer Gutachter für sichere Containerisierung.\n",
    "Bewerte Antworten zu Dockerfile-Review/Erstellung nach diesen Kriterien (0–5, 5 = hervorragend):\n",
    "- correctness: Technische Korrektheit der Aussagen\n",
    "- security: Sicherheitsreife (Least-Privilege, Non-Root, Updates, Supply Chain)\n",
    "- best_practices: Production-Best-Practices (Layering, Multi-Stage, CMD/ENTRYPOINT, Healthcheck, WSGI/ASGI)\n",
    "- clarity: Verständlichkeit, Struktur, Nachvollziehbarkeit\n",
    "- actionability: Konkrete, umsetzbare Vorschläge inkl. Code-Snippets\n",
    "\n",
    "Gib NUR valide JSON zurück mit folgendem Schema:\n",
    "{\n",
    "  \"per_model\": {\n",
    "    \"<model_name>\": {\n",
    "      \"scores\": {\n",
    "        \"correctness\": <0-5>,\n",
    "        \"security\": <0-5>,\n",
    "        \"best_practices\": <0-5>,\n",
    "        \"clarity\": <0-5>,\n",
    "        \"actionability\": <0-5>\n",
    "      },\n",
    "      \"comment\": \"<kurzer Kommentar>\"\n",
    "    },\n",
    "    ...\n",
    "  },\n",
    "  \"ranking\": [\"<best>\", \"<next>\", ...]\n",
    "}\n",
    "Kein weiteres Text-Drumherum, nur JSON.\n",
    "\"\"\"\n",
    "\n",
    "def ask_openai_judge(client, model_name, prompt: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def build_judge_prompt(question: str, answers: dict) -> str:\n",
    "    \"\"\"\n",
    "    answers: { \"gpt-4o-mini\": \"...\", \"llama-3\": \"...\", ... }\n",
    "    \"\"\"\n",
    "    parts = [f\"Frage/Aufgabe:\\n{question}\\n\", \"Antworten der Modelle:\"]\n",
    "    for model_name, answer in answers.items():\n",
    "        parts.append(f\"\\n### {model_name}\\n{answer}\")\n",
    "    parts.append(\"\\nBewerte alle Antworten nach den genannten Kriterien und gib NUR JSON zurück.\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def evaluate_models_with_openai_judge(question: str,\n",
    "                                      answers: dict,\n",
    "                                      openai_client,\n",
    "                                      judge_model_name: str = \"anthropic/claude-opus-4.1\",\n",
    "                                      judge_fn = ask_openai_judge) -> dict:\n",
    "    \"\"\"\n",
    "    question: die gestellte Aufgabe/Frage (z. B. Dockerfile-Review)\n",
    "    answers:  { model_name: model_output, ... }\n",
    "    openai_client: dein bereits initialisierter OpenAI-Client\n",
    "    judge_model_name: welches OpenAI-Modell den Judge spielen soll\n",
    "    judge_fn: Funktions-Handle (standardmäßig ask_openai_judge)\n",
    "    \"\"\"\n",
    "    judge_prompt = build_judge_prompt(question, answers)\n",
    "    raw = judge_fn(openai_client, judge_model_name, judge_prompt)\n",
    "\n",
    "   \n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            data = json.loads(raw[start:end+1])\n",
    "        else:\n",
    "            raise ValueError(f\"Judge antwortete nicht mit JSON. Antwort war:\\n{raw}\")\n",
    "\n",
    "    # Sanity check\n",
    "    if \"per_model\" not in data or not isinstance(data[\"per_model\"], dict):\n",
    "        raise ValueError(f\"JSON hat unerwartete Struktur:\\n{data}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def markdown_score_table(eval_json: dict) -> str:\n",
    "    \"\"\"\n",
    "    eval_json: Struktur wie vom Judge gefordert.\n",
    "    Gibt eine Markdown-Tabelle mit Durchschnittsscore + Kriterien zurück.\n",
    "    \"\"\"\n",
    "    per_model = eval_json.get(\"per_model\", {})\n",
    "    ranking   = eval_json.get(\"ranking\", [])\n",
    "\n",
    "    headers = [\"Modell\", \"Gesamt\", \"correctness\", \"security\", \"best_practices\", \"clarity\", \"actionability\", \"Kommentar\"]\n",
    "    md = [\"|\" + \"|\".join(headers) + \"|\", \"|\" + \"|\".join([\"---\"] * len(headers)) + \"|\"]\n",
    "\n",
    "    # Reihenfolge: ranking zuerst, dann restliche\n",
    "    ordered_models = list(ranking) + [m for m in per_model.keys() if m not in ranking]\n",
    "\n",
    "    for m in ordered_models:\n",
    "        entry = per_model.get(m, {})\n",
    "        scores = entry.get(\"scores\", {})\n",
    "        com = entry.get(\"comment\", \"\")\n",
    "        # Durchschnitt\n",
    "        crits = [\"correctness\",\"security\",\"best_practices\",\"clarity\",\"actionability\"]\n",
    "        vals = [scores.get(c, 0) for c in crits]\n",
    "        avg = sum(vals)/len(vals) if vals else 0.0\n",
    "\n",
    "        row = [\n",
    "            f\"`{m}`\",\n",
    "            f\"{avg:.2f}\",\n",
    "            str(scores.get(\"correctness\",\"\")),\n",
    "            str(scores.get(\"security\",\"\")),\n",
    "            str(scores.get(\"best_practices\",\"\")),\n",
    "            str(scores.get(\"clarity\",\"\")),\n",
    "            str(scores.get(\"actionability\",\"\")),\n",
    "            com.replace(\"\\n\",\" \").strip()\n",
    "        ]\n",
    "        md.append(\"|\" + \"|\".join(row) + \"|\")\n",
    "\n",
    "    # Kurzes Fazit dazu\n",
    "    if ranking:\n",
    "        md.append(f\"\\n**Ranking:** {', '.join([f'`{r}`' for r in ranking])}\")\n",
    "    return \"\\n\".join(md)\n",
    "\n",
    "\n",
    "def compare_answers_markdown(question: str,\n",
    "                             answers: dict,  # {model_name: text}\n",
    "                             openai_client,\n",
    "                             judge_model_name: str = \"gpt-4o-mini\",\n",
    "                             judge_fn = ask_openai_judge,\n",
    "                             display_markdown: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Führt Bewertung aus und gibt eine hübsche Markdown-Tabelle zurück.\n",
    "    Bei display_markdown=True wird die Tabelle direkt gerendert.\n",
    "    \"\"\"\n",
    "    result = evaluate_models_with_openai_judge(\n",
    "        question=question,\n",
    "        answers=answers,\n",
    "        openai_client=openai_client,\n",
    "        judge_model_name=judge_model_name,\n",
    "        judge_fn=judge_fn\n",
    "    )\n",
    "    md = markdown_score_table(result)\n",
    "    if display_markdown:\n",
    "        display(Markdown(md))\n",
    "    return md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237d9ad-b372-4e93-b306-91e9316c61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {\n",
    "    \"deepseek-r1-distill-qwen-7b\": local_ds_dockerfile,\n",
    "    \"llama-3.2-3b-instruct\": local_llama_dockerfile,\n",
    "    \"gpt-4o\": gpt_dockerfile,\n",
    "    \"opus\": opus\n",
    "}\n",
    "\n",
    "_ = compare_answers_markdown(\n",
    "    question=CREATE_PROMPT,\n",
    "    answers=answers,\n",
    "    openai_client=or_client,\n",
    "    judge_model_name=\"google/gemini-3-pro-preview\"   # oder \"gpt-4o\", \"\"anthropic/claude-opus-4.1 falls freigeschaltet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a843d-ef4d-4593-a8e5-e14ec6e498b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heise-classroom-devsecops-ki1125-qbGNGHei-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
